"""
Ø³ÙƒØ±ÙŠØ¨Øª Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ ÙˆØ§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
ÙŠØ¯Ø¹Ù… LSTM Ùˆ Dense Neural Network
"""

import tensorflow as tf
import numpy as np
import json
import os
from pathlib import Path

print("ğŸš€ Ø³ÙƒØ±ÙŠØ¨Øª ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
print("=" * 60)

# Ù‚Ø±Ø§Ø¡Ø© labels.json Ù„ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
labels_file = Path(__file__).parent.parent / "app" / "src" / "main" / "assets" / "labels.json"
if labels_file.exists():
    with open(labels_file, 'r', encoding='utf-8') as f:
        labels = json.load(f)
    num_classes = len(labels)
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {num_classes} ØªØµÙ†ÙŠÙ Ù…Ù† labels.json")
    print(f"   Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª: {', '.join(labels[:5])}... (+ {num_classes - 5} Ø£ÙƒØ«Ø±)")
else:
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
    num_classes = 48  # 28 Ø­Ø±Ù + 20 Ø¥Ø´Ø§Ø±Ø© Ø¬Ø¯ÙŠØ¯Ø©
    print(f"âš ï¸  Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ labels.jsonØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©: {num_classes}")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
INPUT_SIZE = 63  # 21 landmarks Ã— 3 (x, y, z)
SEQUENCE_LENGTH = 10  # Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ù„Ù„Ù€ LSTM
USE_LSTM = True  # True Ù„Ù„Ù€ LSTMØŒ False Ù„Ù„Ù€ Dense

print(f"\nğŸ“Š Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
print(f"   - Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª: {num_classes}")
print(f"   - Ø­Ø¬Ù… Ø§Ù„Ù…Ø¯Ø®Ù„: {INPUT_SIZE} (21 landmarks Ã— 3)")
print(f"   - Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {'LSTM' if USE_LSTM else 'Dense NN'}")
if USE_LSTM:
    print(f"   - Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„: {SEQUENCE_LENGTH}")

def create_lstm_model(num_classes: int, sequence_length: int = 10):
    """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ LSTM"""
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(sequence_length, INPUT_SIZE)),
        tf.keras.layers.LSTM(256, return_sequences=True, name='lstm_1'),
        tf.keras.layers.Dropout(0.3, name='dropout_1'),
        tf.keras.layers.LSTM(128, return_sequences=True, name='lstm_2'),
        tf.keras.layers.Dropout(0.3, name='dropout_2'),
        tf.keras.layers.LSTM(64, name='lstm_3'),
        tf.keras.layers.Dropout(0.2, name='dropout_3'),
        tf.keras.layers.Dense(128, activation='relu', name='dense_1'),
        tf.keras.layers.Dropout(0.2, name='dropout_4'),
        tf.keras.layers.Dense(64, activation='relu', name='dense_2'),
        tf.keras.layers.Dense(num_classes, activation='softmax', name='output')
    ])
    return model

def create_dense_model(num_classes: int):
    """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Dense Neural Network"""
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(INPUT_SIZE,)),
        tf.keras.layers.Dense(256, activation='relu', name='dense_1'),
        tf.keras.layers.Dropout(0.3, name='dropout_1'),
        tf.keras.layers.Dense(128, activation='relu', name='dense_2'),
        tf.keras.layers.Dropout(0.3, name='dropout_2'),
        tf.keras.layers.Dense(64, activation='relu', name='dense_3'),
        tf.keras.layers.Dropout(0.2, name='dropout_3'),
        tf.keras.layers.Dense(num_classes, activation='softmax', name='output')
    ])
    return model

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print(f"\nğŸ”§ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
if USE_LSTM:
    model = create_lstm_model(num_classes, SEQUENCE_LENGTH)
    model_name = "arabic_sign_lstm.tflite"
else:
    model = create_dense_model(num_classes)
    model_name = "arabic_sign_dense.tflite"

# ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print("\nğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
model.summary()

# Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
total_params = model.count_params()
print(f"\nğŸ“ˆ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {total_params:,}")

# Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ (Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¹Ù…Ù„)
print("\nğŸ§ª Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©...")
if USE_LSTM:
    # Ø¨ÙŠØ§Ù†Ø§Øª LSTM: [batch_size, sequence_length, features]
    X_dummy = np.random.random((100, SEQUENCE_LENGTH, INPUT_SIZE))
    y_dummy = np.random.random((100, num_classes))
else:
    # Ø¨ÙŠØ§Ù†Ø§Øª Dense: [batch_size, features]
    X_dummy = np.random.random((100, INPUT_SIZE))
    y_dummy = np.random.random((100, num_classes))

# Normalize labels to probabilities
y_dummy = y_dummy / y_dummy.sum(axis=1, keepdims=True)

# ØªØ¯Ø±ÙŠØ¨ ØªØ¬Ø±ÙŠØ¨ÙŠ (epoch ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
print("\nğŸ‹ï¸  ØªØ¯Ø±ÙŠØ¨ ØªØ¬Ø±ÙŠØ¨ÙŠ (epoch ÙˆØ§Ø­Ø¯ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)...")
history = model.fit(
    X_dummy, 
    y_dummy, 
    epochs=1, 
    verbose=1,
    validation_split=0.2
)

# ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ TFLite
print("\nğŸ”„ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ TFLite...")
try:
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    # LSTM ÙŠØªØ·Ù„Ø¨ Select TF Ops
    if USE_LSTM:
        converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS,
            tf.lite.OpsSet.SELECT_TF_OPS
        ]
        converter._experimental_lower_tensor_list_ops = False
        print("   â„¹ï¸  Ø§Ø³ØªØ®Ø¯Ø§Ù… Select TF Ops Ù„Ù„Ù€ LSTM")
    
    tflite_model = converter.convert()
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    output_file = Path(__file__).parent / model_name
    with open(output_file, 'wb') as f:
        f.write(tflite_model)
    
    file_size_kb = len(tflite_model) / 1024
    file_size_mb = file_size_kb / 1024
    
    print(f"\nâœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"   ğŸ“ Ø§Ù„Ù…Ù„Ù: {output_file}")
    print(f"   ğŸ“¦ Ø§Ù„Ø­Ø¬Ù…: {file_size_kb:.2f} KB ({file_size_mb:.2f} MB)")
    print(f"\nğŸ“‹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:")
    print(f"   1. Ø§Ù†Ø³Ø® Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰: app/src/main/assets/{model_name}")
    print(f"   2. Ø£Ø¹Ø¯ Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ (Clean & Rebuild)")
    print(f"   3. Ø§Ø®ØªØ¨Ø± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    
    print(f"\nâš ï¸  Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©:")
    print(f"   Ù‡Ø°Ø§ Ù†Ù…ÙˆØ°Ø¬ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙÙ‚Ø·!")
    print(f"   Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠØŒ Ø¯Ø±Ù‘Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©:")
    print(f"   - Ø§Ø³ØªØ®Ø¯Ù… Google Colab Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
    print(f"   - Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ù…Ù† dataset Ø­Ù‚ÙŠÙ‚ÙŠ")
    print(f"   - Ø±Ø§Ø¬Ø¹ SETUP_MODELS.md Ù„Ù„ØªÙØ§ØµÙŠÙ„")
    
except Exception as e:
    print(f"\nâŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
    print(f"   ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª TensorFlow Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­")

print("\n" + "=" * 60)
print("âœ… Ø§ÙƒØªÙ…Ù„!")

